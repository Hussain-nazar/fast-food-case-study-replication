# -*- coding: utf-8 -*-
"""FeynnLabProject1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mFYIWk1Y7yYGyqi1bSHzHYjbcUo2d0aV

I(Ajeet) have run this file We can work together in this collab
"""

# Importing necessary libraries
import pandas as pd  # For data manipulation and analysis
from sklearn.decomposition import PCA  # For Principal Component Analysis (PCA) to reduce dimensions
import numpy as np  # For numerical operations
from sklearn import preprocessing  # For scaling and normalization of data
from bioinfokit.visuz import cluster  # For data visualization, specifically clustering
import matplotlib.pyplot as plt  # For plotting graphs
from sklearn.cluster import KMeans  # For K-Means clustering algorithm
from sklearn.utils import resample  # For resampling methods like bootstrapping
from sklearn.metrics import adjusted_rand_score  # For evaluating clustering results using Adjusted Rand Index

pip install bioinfokit #Install the bioinfokit package which provides tools for bioinformatics analysis and visualization

# Reading the McDonald's dataset from the CSV file into a DataFrame
data = pd.read_csv("mcdonalds.csv")  # Load the dataset into 'data' DataFrame

# Reading the same dataset into a second DataFrame
data1 = pd.read_csv("mcdonalds.csv")  # Load the dataset again into 'data1' DataFrame

# Displaying the list of column names in the dataset
data.columns.values.tolist()  # Get the column names of the dataset as a list

# Getting the shape of the dataset, which returns the number of rows and columns
data.shape

# Displaying the first 3 rows of the dataset to inspect the data
data.head(3)

# Selecting the first 11 columns of the dataset and replacing 'Yes' with 1 and 'No' with 0
MD=data.iloc[:,0:11].replace("Yes",1).replace("No",0)
# Calculating the mean of each column in the transformed DataFrame, rounded to 2 decimal places
mean=round(MD.mean(),2)
mean   # Output the mean values

# Initializing PCA (Principal Component Analysis) object
pca = PCA()
# Applying PCA on the MD data and transforming it to a new space with fewer dimensions
MD_pca=pca.fit_transform(MD)
# Fitting PCA on the MD data to learn the principal components (without transforming the data yet)
MD_p=pca.fit(MD)

# Calculating the standard deviation for each principal component
SD=np.sqrt(pca.explained_variance_)
# Calculating the proportion of variance explained by each principal component
PV=pca.explained_variance_ratio_
# Creating a list of principal component names (e.g., 'PC1', 'PC2', etc.)
index=[]
for i in range(len(SD)):
    i=i+1
    index.append("PC{}".format(i))

# Creating a DataFrame to summarize standard deviations, proportion of variance, and cumulative variance
sum=pd.DataFrame({
    "Standard deviation":SD,"Proportion of Variance":PV,"Cumulative Proportion":PV.cumsum()
},index=index)
sum   #Display the summary DataFrame

# Printing the standard deviation of each principal component, rounded to 1 decimal place
print("Standard Deviation:\n",SD.round(1))

# Getting the loadings (principal component coefficients)
load = (pca.components_)
# Extracting the rotation matrix (principal components in the original space)
i=0
rot_matrix = MD_p.components_.T

# Creating a DataFrame to display the rotation matrix, with column names as principal components
rot_df = pd.DataFrame(rot_matrix, index=MD.columns.values, columns=index)
# Rounding the rotation matrix values to 3 decimal places and flipping the signs
rot_df=round(-rot_df,3)
rot_df   # Display the rotated matrix DataFrame

# Displaying the rotation matrix (loadings of original features for each principal component)
rot_df

cluster.biplot(cscore=MD_pca, loadings=-load, labels=data.columns.values,var1=0,var2=0, show=True, dim=(10, 10))

np.random.seed(1234)

nrep = 10

num_segments = range(1, 9)
within_cluster_distances = []
MD_km28 = {}

for k in num_segments:
    kmeans = KMeans(n_clusters=k, n_init=nrep, random_state=1234)
    kmeans.fit(MD)
    within_cluster_distances.append((kmeans.inertia_))
    MD_km28[str(k)] = kmeans

plt.bar(num_segments, within_cluster_distances)
plt.xlabel("Number of segments")
plt.ylabel("Sum of within-cluster distances")
plt.title("Segmentation Results")
plt.show()

np.random.seed(1234)
nboot = 100
nrep = 10

bootstrap_samples = []
for _ in range(nboot):
    bootstrap_sample = resample(MD.values, random_state=1234)
    bootstrap_samples.append(bootstrap_sample)

adjusted_rand_index = []
num_segments = range(2, 9)
for k in num_segments:
    stability_scores = []
    for bootstrap_sample in bootstrap_samples:
        kmeans = KMeans(n_clusters=k, n_init=nrep, random_state=1234)
        kmeans.fit(bootstrap_sample)
        cluster_labels = kmeans.predict(bootstrap_sample)
        true_labels = kmeans.predict(MD.values)
        stability_score = adjusted_rand_score(true_labels, cluster_labels)
        stability_scores.append(stability_score)
    adjusted_rand_index.append(stability_scores)

# Transpose the adjusted_rand_index list
adjusted_rand_index = np.array(adjusted_rand_index).T

# Create boxplot of adjusted Rand index
plt.boxplot(adjusted_rand_index, labels=num_segments, whis=10)
plt.xlabel("Number of segments")
plt.ylabel("Adjusted Rand Index")
plt.title("Bootstrap Flexclust")
plt.show()

range_values = (0, 1)
num_bins = 10
max_frequency = 200


fig, axs = plt.subplots(2, 2, figsize=(12, 8))


for i in range(1, 5):
    labels = MD_km28[str(i)].predict(MD)
    similarities = MD_km28[str(i)].transform(MD).min(axis=1)
    row = (i - 1) // 2
    col = (i - 1) % 2

    axs[row, col].hist(similarities, bins=num_bins, range=range_values)
    axs[row, col].set_xlabel('Similarity')
    axs[row, col].set_ylabel('Frequency')
    axs[row, col].set_title('cluster {}'.format(i))

    axs[row, col].set_xlim(range_values)
    axs[row, col].set_ylim(0, max_frequency)


    axs[row, col].set_xticks([0, 0.2, 0.4, 0.6, 0.8, 1.0])

plt.tight_layout()
plt.show()

num_segments = range(2, 9)

segment_stability = []
for segment in range(2, 9):
    labels_segment = MD_km28[str(segment)].predict(MD)
    segment_stability.append(labels_segment)

plt.figure(figsize=(8, 6))
for i, segment in enumerate(range(2, 9)):
    plt.plot(num_segments, [np.mean(segment_stability[i] == labels) for labels in segment_stability], marker='o', label=f'Segment {segment}')

plt.xlabel('Number of Segments')
plt.ylabel('Segment Level Stability')
plt.title('Segment Level Stability Across Solutions (SLSA) Plot')
plt.xticks(num_segments)
plt.legend()
plt.grid(True)

plt.show()

segment_solutions = ["2", "3", "4", "5"]
segment_labels = {}
segment_similarities = {}

for segment in segment_solutions:
    segment_labels[segment] = MD_km28[segment].predict(MD)
    segment_similarities[segment] = MD_km28[segment].transform(MD).min(axis=1)

segment_stability_values = []
for segment in segment_solutions:
    similarities = segment_similarities[segment]
    normalized_similarities = similarities / np.max(similarities)
    segment_stability_values.append(normalized_similarities)

plt.boxplot(segment_stability_values, whis=1.5)
plt.xlabel("Segment Number")
plt.ylabel("Segment Stability")
plt.xticks(range(1, len(segment_solutions) + 1), segment_solutions)
plt.ylim(0, 1)
plt.title("Segment Level Stability within Solutions")

plt.show()

from scipy.stats import entropy

np.random.seed(1234)
k_values = range(2, 9)
MD_m28 = []

for k in k_values:
    model = KMeans(n_clusters=k, random_state=1234)
    model.fit(MD.values)
    iter_val = model.n_iter_
    converged = True
    k_val = k
    k0_val = k
    log_likelihood = -model.inertia_
    n_samples, _ = MD.shape
    aic = -2 * log_likelihood + 2 * k
    bic = -2 * log_likelihood + np.log(n_samples) * k
    labels = model.labels_
    counts = np.bincount(labels)
    probs = counts / float(counts.sum())
    class_entropy = entropy(probs)
    icl = bic - class_entropy

    MD_m28.append((iter_val, converged, k_val, k0_val, log_likelihood, aic, bic, icl))
MD_m28 = pd.DataFrame(MD_m28, columns=['iter', 'converged', 'k', 'k0', 'logLik', 'AIC', 'BIC', 'ICL'])

print(MD_m28)

num_segments = MD_m28["k"]
AIC_values = MD_m28["AIC"]
BIC_values = MD_m28["BIC"]
ICL_values = MD_m28["ICL"]

plt.plot(num_segments, AIC_values, marker='o', label='AIC')
plt.plot(num_segments, BIC_values, marker='o', label='BIC')
plt.plot(num_segments, ICL_values, marker='o', label='ICL')

plt.xlabel('Number of Segments')
plt.ylabel('Value of Information Criteria')
plt.title('Information Criteria (AIC, BIC, ICL)')
plt.legend()
plt.grid(True)

plt.show()

from sklearn.mixture import GaussianMixture
k = 4
kmeans = KMeans(n_clusters=k, random_state=1234)
kmeans.fit(MD)
kmeans_clusters = kmeans.predict(MD)

gmm = GaussianMixture(n_components=k, random_state=1234)
gmm.fit(MD)
gmm_clusters = gmm.predict(MD)

results = pd.DataFrame({'kmeans': kmeans_clusters, 'mixture': gmm_clusters})

MD_m4 = MD[results['mixture'] == 3]

k4_m4 = KMeans(n_clusters=k, random_state=1234)
k4_m4.fit(MD_m4)
k4_m4_clusters = k4_m4.predict(MD_m4)

results_m4 = pd.DataFrame({'kmeans': k4_m4_clusters, 'mixture': 3})

print(pd.crosstab(results['kmeans'], results['mixture']))
print(pd.crosstab(results['kmeans'], results_m4['kmeans']))

from sklearn.mixture import GaussianMixture
import numpy as np

gmm_m4a = GaussianMixture(n_components=4)
gmm_m4a.fit(MD)

log_likelihood_m4a = gmm_m4a.score(MD)

gmm_m4 = GaussianMixture(n_components=4)
gmm_m4.fit(MD)

log_likelihood_m4 = gmm_m4.score(MD)

print("Log-likelihood for MD.m4a:", log_likelihood_m4a)
print("Log-likelihood for MD.m4:", log_likelihood_m4)

like_counts = pd.value_counts(data['Like'])
reversed_counts = like_counts.iloc[::-1]

print(reversed_counts)

# Define a mapping of string values to numeric codes
like_mapping = {
    'I HATE IT!-5': -5,
    '-4': -4,
    '-3': -3,
    '-2': -2,
    '-1': -1,
    '0': 0,
    '1': 1,
    '2': 2,
    '3': 3,
    '4': 4,
    'I LOVE IT!+5': 5
}

data['Like.n'] = data['Like'].map(like_mapping)


like_n_counts = data['Like.n'].value_counts()


print(like_n_counts)

from patsy import dmatrices

independent_vars = data.columns[0:11]

formula_str = ' + '.join(independent_vars)

formula_str = 'Like ~ ' + formula_str


f = dmatrices(formula_str, data=data)[1]

print(f)

from sklearn.mixture import GaussianMixture
from patsy import dmatrix
np.random.seed(1234)

X = dmatrix(f.design_info, data=data)
y = dmatrix('Like', data=data)

n_components = 2
n_init = 10
verbose = False
n_rep=10

model = GaussianMixture(n_components=n_components, n_init=n_init, verbose=verbose)
MD_reg2 = model.fit(X, y)

print(MD_reg2)
cluster_sizes = np.bincount(model.predict(X))

print("Cluster sizes:")
for i, size in enumerate(cluster_sizes):
    print(f"{i+1}: {size}")

import pandas as pd
# Importing pandas for data manipulation

import matplotlib.pyplot as plt
# Importing matplotlib for plotting
kmeans = MD_km28['4']

import pandas as pd
import matplotlib.pyplot as plt

kmeans = MD_km28['4']

labels = kmeans.labels_

MD_mean = MD.groupby(labels).mean()

fig, axs = plt.subplots(2, 2, figsize=(10, 6))
axs[0, 0].barh(range(MD_mean.shape[1]), MD_mean.iloc[0])
axs[0, 0].set_title('Component 1')
axs[0, 1].barh(range(MD_mean.shape[1]), MD_mean.iloc[1])
axs[0, 1].set_title('Component 2')
axs[1, 0].barh(range(MD_mean.shape[1]), MD_mean.iloc[2])
axs[1, 0].set_title('Component 3')
axs[1, 1].barh(range(MD_mean.shape[1]), MD_mean.iloc[3])
axs[1, 1].set_title('Component 4')

for ax in axs.flat:
    ax.set(ylabel='Variable', xlabel='Proportion')
    ax.set_yticks(range(MD_mean.shape[1]))
    ax.set_yticklabels(MD.columns)

for ax in axs.flat:
    ax.label_outer()

fig.suptitle('Segment Profiles')

fig.tight_layout()

plt.show()

from sklearn.cluster import KMeans # Importing KMeans clustering algorithm from sklearn

from sklearn.decomposition import PCA # Importing PCA (Principal Component Analysis) for dimensionality reduction
import matplotlib.pyplot as plt # Importing matplotlib for plotting
kmeans = KMeans(n_clusters=4) # Creating a KMeans model with 4 clusters
kmeans.fit(MD) # Fitting the KMeans model on the dataset 'MD'
pca = PCA(n_components=2) # Initializing PCA to reduce data to 2 principal components
MD_pca = pca.fit_transform(MD) # Applying PCA on 'MD' to transform it into 2D data for visualization
fig, ax = plt.subplots() # Creating a new matplotlib figure and axis
ax.scatter(MD_pca[:, 0], MD_pca[:, 1]) # Creating a scatter plot using the two principal components
ax.set_xlabel('principal component 1') # Labeling the x-axis as the first principal component
ax.set_ylabel('principal component 2') # Labeling the y-axis as the second principal component
plt.show() # Displaying the scatter plot

from statsmodels.graphics.mosaicplot import mosaic # Importing mosaic plot for visualizing categorical data
from itertools import product # Importing product (not used here, but often for generating combinations)
#Label encoding for categorical - Converting 11 cols with yes/no
from sklearn.preprocessing import LabelEncoder # Importing LabelEncoder to convert categorical text data into numbers
def labelling(x): # Defining a function to apply label encoding on a given column

    data1[x] = LabelEncoder().fit_transform(data1[x])
    return data1

cat = ['yummy', 'convenient', 'spicy', 'fattening', 'greasy', 'fast', 'cheap',

       'tasty', 'expensive', 'healthy', 'disgusting'] # List of 11 categorical features (yes/no) related to food opinions


for i in cat:

    labelling(i) # Applying label encoding to each column in the list
data1 # Displaying the modified DataFrame with encoded categorical values
df_eleven = data1.loc[:,cat] # Extracting the 11 encoded columns into a new DataFrame
df_eleven # Displaying the new DataFrame containing only the 11 opinion features
kmeans = KMeans(n_clusters=4, init='k-means++', random_state=0).fit(df_eleven) # Applying KMeans clustering with 4 clusters on the 11 opinion features
data1['cluster_num'] = kmeans.labels_ # Applying KMeans clustering with 4 clusters on the 11 opinion features
crosstab =pd.crosstab(data1['cluster_num'],data1['Like']) # Creating a crosstab of clusters vs. Like responses for analysis
#Reordering cols
data1 # Displaying the updated DataFrame with cluster assignments
crosstab = crosstab[['I hate it!-5','-4','-3','-2','-1','0','1','2','3','4','I love it!+5']] # Reordering the 'Like' columns in the crosstab for better visual arrangement
crosstab # Displaying the reordered crosstab
plt.rcParams['figure.figsize'] = (7,5) # Setting the default size for plots
mosaic(crosstab.stack()) # Creating a mosaic plot to show the distribution of 'Like' responses across clusters
plt.show()# Displaying the mosaic plot

from statsmodels.graphics.mosaicplot import mosaic # Importing the mosaic plot function from statsmodels for categorical data visualization
MD_k4=MD_km28['4'] # Extracting the clustering model for 4 clusters from the dictionary MD_km28
k4 = MD_k4.labels_ # Getting the cluster labels assigned by the k-means model for each data point
ct = pd.crosstab(k4, data['Gender']) # Creating a cross-tabulation table of cluster labels vs. gender counts
ct # Displaying the cross-tabulation table
mosaic(ct.stack(),gap=0.01) # Plotting a mosaic plot to visually compare gender distribution across clusters with minimal gap
plt.show() # Displaying the mosaic plot

df = pd.DataFrame({'Segment': k4, 'Age': data['Age']}) # Creating a new DataFrame with 'Segment' labels (from k-means result k4) and corresponding 'Age' values
df.boxplot(by='Segment', column='Age') # Creating a box-and-whisker plot of 'Age' grouped by 'Segment' to show age distribution across segments
plt.title('Parallel box-and-whisker plot of age by segment') # Setting the title for the plot
plt.suptitle('') # Removing the default subplot title to keep the plot clean
plt.show() # Displaying the box plot

data1['VisitFrequency'] = LabelEncoder().fit_transform(data1['VisitFrequency'])
# Encoding the 'VisitFrequency' column into numeric values using LabelEncoder
visit = data1.groupby('cluster_num')['VisitFrequency'].mean()
# Calculating the average visit frequency for each cluster
visit = visit.to_frame().reset_index()
# Converting the result to a DataFrame and resetting the index
visit
# Displaying the DataFrame showing average visit frequency per cluster

#Like
data1['Like'] = LabelEncoder().fit_transform(data1['Like']) # Encoding the 'Like' column into numeric values using LabelEncoder (e.g., Like = 1, Dislike = 0)
Like = data1.groupby('cluster_num')['Like'].mean()# Calculating the average 'Like' score for each cluster to understand cluster-wise preference
Like = Like.to_frame().reset_index() # Converting the result into a DataFrame and resetting the index
Like # Displaying the DataFrame with average 'Like' scores per cluster

data1['Gender'] = LabelEncoder().fit_transform(data1['Gender']) # Encoding the 'Gender' column as numerical values (e.g., Male = 1, Female = 0) using LabelEncoder
Gender = data1.groupby('cluster_num')['Gender'].mean()# Calculating the mean gender value for each cluster group (helps indicate gender distribution in clusters)
Gender = Gender.to_frame().reset_index()# Converting the Series to a DataFrame and resetting the index for clarity
Gender # Displaying the final DataFrame showing average gender per cluster

import seaborn as sns # Importing the seaborn library for statistical data visualization
segment = Gender.merge(Like, on='cluster_num', how='left').merge(visit, on='cluster_num', how='left') # Merging the 'Gender', 'Like', and 'visit' DataFrames on the 'cluster_num' column using left join to create a consolidated 'segment' DataFrame

segment# Displaying the merged DataFrame
plt.figure(figsize = (9,4))# Creating a new figure with a specific size of 9 inches by 4 inches
sns.scatterplot(x = "VisitFrequency", y = "Like",data=segment,s=400, color="r") # Plotting a scatter plot using seaborn with 'VisitFrequency' on x-axis and 'Like' on y-axis, red color and size 400 for points
plt.title("Simple segment evaluation plot for the fast food data set",fontsize = 15)# Adding a title to the plot with font size 15
plt.xlabel("Visit", fontsize = 12)# Labeling the x-axis as "Visit" with font size 12
plt.ylabel("Like", fontsize = 12)# Labeling the y-axis as "Like" with font size 12
plt.show()# Displaying the final plot

